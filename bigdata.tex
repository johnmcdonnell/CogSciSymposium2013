% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

\documentclass[10pt,letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{soul}
\usepackage{color}


\title{A Data Revolution in the Cognitive Sciences}
 
\author{{\large \bf John V. McDonnell (\emph{moderator}), Todd M. Gureckis, and
Matthew J. C. Crump}\\ New York University and Brooklyn College of CUNY 
  \AND {\large \bf Johan Bolen} \\
  Indiana University School of Informatics and Computing
  \AND {\large \bf Georg Langs} \\
  Medical University of Vienna
  \AND {\large \bf Winter Mason, Meeyoung Cha, Krishna Gummadi, Farshad Kooti, and Haeryun Yang} \\
  Howe School of Technology Managemeent, Stevens Institute 
  \AND {\large \bf John Myles White} \\
  Princeton University Department of Psychology }


\begin{document}

\maketitle

\begin{footnotesize}
\textbf{Keywords:} 
Machine Learning; Cognitive Neuroscience; Online Experiments; Social Network
Analysis; Text Analysis; Methods
\end{footnotesize}

In the 1980s, the broad availability of personal computers fueled a renaissance
in cognitive science, facilitating the development of sophisticated computational
models and changing the way we run experiments and analyze data. As technology
continues to advance, the advent of so-called \emph{big data} (massive scale data
made accessible by distributed networks) is leading to methodological advances on
various fronts, from online experimentation to large scale data mining. The goal
of this symposium is to highlight how researchers are making use of these new
tools to make advances in cognitive science.

A major theme of the symposium is that these new techniques represent more than
just a scaling up of previous efforts. Big data is not just the latest effect of
our expanding hard drive storage capacities. The explosion of data has allowed
researchers to ask entirely new questions. For example, collective behavior has
long been of interest to cognitive scientists but was difficult to study directly
on a large scale. Now that scientists have access to data from massive social
networks, these interactions can be studied in naturalistic settings such as
Facebook and Twitter. Johan Bolen and Winter Mason et al. will discuss innovative
work on the emergence of social conventions in large networks.

Another novel source of data are online labor marketplaces such as Amazon's
Mechanical Turk (AMT) service. These allow researchers to perform controlled
studies involving large numbers of participants in short periods of time. John
Myles White and John McDonnell et al. will talk about their work running massive
experimental studies online with AMT.

Finally, cognitive neuroscience has also been a major beneficiary of the big data
revolution. Analysis of large volumes of imaging data poses challenges in terms
of statistical rigor and managing immense quantities of data. Georg Langs will
speak about his work using cutting edge machine learning techniques to derive new
insights from imaging data.


\section{Big data for computational social science: Social networks and sentiment}
\subsubsection{Johan Bollen}

Twitter and Facebook now have more than one billion users combined. These social
media users produce hundreds of millions of messages each day that provide a
unique window into our collective thoughts, feelings, experiences, and
observations. Social media data is uniquely valuable not merely because of its
scale, coverage, and ability to gauge ephemeral personal conditions, but because
it provides a record of long-term social ties and demographic features. The
resulting data enables the study of a variety of socio-economic phenomena at the
intersection of psychology, sociology, linguistics, biology, and computer
science.

Our research is particularly focused on the role that human mood and sentiment
play in collective intelligence and decision-making. At the individual level our
decision-making is strongly influenced by emotions. This may be even more the
case at a societal level where social relations and mass communication amplify
the production and propagation of emotive information and misinformation.

In past work we have performed large-scale analysis of collective mood states
using computationally extended versions of existing psychometric models. The
results provide a quantitative assessment of the fluctuations of societal
well-being and mood over time that can be leveraged to study a variety of
sociological phenomena such as mood homophily, mood contagion, meme propagation,
and the effect of public mood states on economic and financial indicators. More
recent work has focused on the development of computational models of the
relations between social mood and socio-economic indicators, as well as the
analysis of collective knowledge representations and their role in the
propagation of misinformation and political bifurcation processes.


\section{Machine Learning Approaches in Neuroimaging and Medical Image Analysis.}
\subsubsection{Georg Langs}

The computational analysis of biomedical imaging data has become central in
medical research and clinical application. The talk will highlight two recent
developments that might shape research in this area in the near future. Both are
connected to the impact of the emerging ability to process large data, and the
approaches that become relevant. The advent of the capability to process large
data---hundreds of terabytes---has changed paradigms in how we approach classical
pattern recognition problems in this domain. Medical research fields that rely on
exploratory approaches together with large and complex data to study and
understand physiological processes are increasingly shaped by a tight interaction
among researchers in machine learning and medical sciences. In the talk I will
discuss these directions, highlight open issues, and illustrate them
with examples from medical image retrieval, neuroimaging research, and
clinical application.

\section{The emergence of social conventions in social networks}
\subsubsection{Winter Mason, Meeyoung Cha, Krishna Gummadi, Farshad Kooti, and Haeryun Yang}

Social conventions and norms are a powerful and ubiquitous guide for behavior.
However, the way in which conventions emerge in communities is not very well
understood, largely due to a paucity of available data.  In this study, we
leverage the widespread use of the micro-blogging platform Twitter and focus on
competing conventions for attributing reposts to the original source.  We analyze
over 1.7 billion ``tweets'' from 54 million users, from the very first tweet in
2006 up to September 2009, and observe how the conventions emerged and spread
through the network of Twitter users.  We observe that initially the most
successful conventions were borrowed from natural language (``via'' and
``retweeting''), but eventually a community-specific convention came to dominate
(``RT'').  We see that some conventions are used by divergent groups of people,
while others are abandoned in favor of more efficient expressions.  We also
observe the failure of some conventions despite higher efficiency (i.e., fewer
characters) and explicit endorsement of their adoption.  Our results suggest that
there are some features that encourage the adoption of one convention over
another, but that there is still significant inherent unpredictability in what
convention will come to dominate.

\section{Context and decision making in a massive online experiment}
\subsubsection{John Myles White}

Online labor markets, like Amazon's Mechanical Turk (AMT), offer psychologists
many opportunities.  The convenience of the virtual lab provided by AMT has
already won over many psychologists, but transitioning research from the lab to
the web browser offers other benefits. AMT allows psychologists to deploy
experiments that are fully automated: Recruitment, instruction, core
experiment and debriefing periods can be identical for all subjects, which
largely removes the possibility that undocumented components of an experiment
might exert substantive influences on the final results (Doyen et al., 2012).
\nocite{Doyen} Moreover, the ease of recruiting large numbers of subjects for
tasks provides important increases in statistical power, which can allay concerns
about false positive psychology (Simmons et al., 2012).  \nocite{Simmons} 

But I will argue that the primary value of AMT is not purely methodological: the
strength of the virtual lab is that it allows psychologists to pursue large-scale
between-subjects designs in which a large number of subjects perform a very small
number of trials.  This type of research is often eschewed because of the
prohibitive cost of recruiting hundreds or thousands of subjects in the lab, but
previous research (e.g. Gneezy et al. 2006) \nocite{Gneezy} suggests that studies
of decision-making can be powerfully influenced by contextual cues.  Our recent
work has found evidence that the effects of context on decisions may be even more
problematic than previously believed: in our studies of decision-making, we find
that the effects of ostensibly innocuous local context can shift the basic
qualitative results of experiments.

\section{Can online data be trusted? Learning tasks on Amazon's Mechanical Turk.}
\subsubsection{John V. McDonnell, Todd M. Gureckis, and Matthew J. C. Crump}

Amazon Mechanical Turk (AMT) has attracted attention from experimental
psychologists interested in gathering human subject data more efficiently.
However, relative to traditional laboratory studies, many aspects of the testing
environment are not under the experimenter's control. We have empirically
evaluated the fidelity of AMT for use in cognitive behavioral experiments. These
types of experiment differ from simple surveys in that they require multiple
trials with sustained attention from participants.  Our initial attempts to
replicate the classic Shepard, Hovland, and Jenkins (1961) task were only
partially successful. However, after systematically studying the effects of
compensation and validation we were able to more closely match previous in-lab
findings. Specifically, we found that compensation altered the rate of sign-ups,
but not the quality of the data. Conversely, we found that using a strict measure
to ensure that participants had understood instructions resulted in considerable
improvement in performance and convergence with in-lab findings.
\nocite{Shepard:1961wo}




\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{symposium}


\end{document}
